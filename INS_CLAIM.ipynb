{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eae97973-c176-4ea7-bca3-2500fc6733e8",
   "metadata": {},
   "source": [
    "## Insurance Claim Prediction Project Report\n",
    "______________________________________________\n",
    "\n",
    "## Project Description\n",
    "\n",
    "This project predicts which customers are likely to make an insurance claim using machine learning. The goal is to help the client reduce risk and improve claim handling.\n",
    "\n",
    "We used LightGBM classifier because it works well with large datasets, handles missing values automatically, and is fast with n_jobs=-1 using all CPU cores. All -1 values were replaced with NaN so that the model can handle missing data properly.\n",
    "\n",
    "The dataset was split into train, validation, and test sets. Validation set and early stopping help prevent overfitting. Multiple evaluation metrics were calculated: ROC AUC, PR AUC, accuracy, precision, recall, F1 score, and MCC.\n",
    "\n",
    "Note: Exploratory Data Analysis (EDA) was not done because the client requested to skip it. Despite this, the model learned patterns from raw features.\n",
    "\n",
    "Model Evaluation\n",
    "Metric\tScore\tNote\n",
    "ROC AUC\t0.6369\tMeasures probability ranking\n",
    "PR AUC\t0.0659\tVery low due to imbalance\n",
    "Accuracy\t0.6456\tOverall correct predictions\n",
    "Precision\t0.0554\tMany predicted claims are false\n",
    "Recall\t0.5434\tAbout half of actual claims caught\n",
    "F1 Score\t0.1005\tBalance of precision and recall\n",
    "MCC\t0.0754\tConfirms effect of imbalanced data\n",
    "\n",
    "Confusion Matrix:\n",
    "\n",
    "True Negatives: 74498\n",
    "\n",
    "False Positives: 40206\n",
    "\n",
    "False Negatives: 1981\n",
    "\n",
    "True Positives: 2358"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa68bff5-115b-42fd-9062-47a5e1253899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n",
      "[LightGBM] [Info] Number of positive: 13884, number of negative: 367051\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.058371 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1367\n",
      "[LightGBM] [Info] Number of data points in the train set: 380935, number of used features: 57\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=-0.000000\n",
      "[LightGBM] [Info] Start training from score -0.000000\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[466]\tvalid_0's auc: 0.640582\tvalid_0's binary_logloss: 0.635482\n",
      "------- MODEL EVALUATION REPORT -------\n",
      "ROC AUC Score: 0.6369\n",
      "PR AUC Score:  0.0659\n",
      "Accuracy:      0.6456\n",
      "Precision:     0.0554\n",
      "Recall:        0.5434\n",
      "F1 Score:      0.1005\n",
      "MCC Score:     0.0754\n",
      "\n",
      "------- CONFUSION MATRIX -------\n",
      "True Negatives (Correct No-Claim): 74498\n",
      "False Positives (False Alarm):     40206\n",
      "False Negatives (Missed Claim):    1981\n",
      "True Positives (Caught Claim):     2358\n",
      "\n",
      "------- CLASSIFICATION REPORT -------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.65      0.78    114704\n",
      "           1       0.06      0.54      0.10      4339\n",
      "\n",
      "    accuracy                           0.65    119043\n",
      "   macro avg       0.51      0.60      0.44    119043\n",
      "weighted avg       0.94      0.65      0.75    119043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd                # for data handling \n",
    "import numpy as np                 # for numerical operations \n",
    "import lightgbm as lgb             # for importing machine learning model\n",
    "from sklearn.model_selection import train_test_split   # for spliting data into train and test\n",
    "\n",
    "df = pd.read_csv(\"InsClaim.csv\")  # load dataset \n",
    "\n",
    "df = df.replace(-1, np.nan)       # replace missing -1 with nan \n",
    "\n",
    "X = df.drop(['target', 'id'], axis=1)  # features and removing unwanted id column\n",
    "y = df['target']                       # target variable \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y  # Spliting train data = 80% and test data = 20%\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train  # validation split \n",
    ")\n",
    "\n",
    "model = lgb.LGBMClassifier(\n",
    "    class_weight='balanced',   # handle imbalanced data because we are not using SMOTE\n",
    "    n_estimators=2000,          # selecting total number of trees = 2000 (\n",
    "    learning_rate=0.01,         # selecting learning rate as 0.01 for slow learning and stable result\n",
    "    num_leaves=50,              # number of leaves will be 50\n",
    "    random_state=42,            # for getting fix output every time\n",
    "    n_jobs=-1                   # CPU will use every core (fast processing)\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_val, y_val)],      # validate during training\n",
    "    eval_metric='auc',              # metric to check\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=100)]  # if no improvement found till 100 itterations stop it.\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    "    matthews_corrcoef,\n",
    "    average_precision_score\n",
    ")                                # importing all metrics\n",
    "\n",
    "y_pred = model.predict(X_test)            # predictions \n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # predicted probabilities \n",
    "\n",
    "print(\"------- MODEL EVALUATION REPORT -------\")  \n",
    "print(f\"ROC AUC Score: {roc_auc_score(y_test, y_pred_proba):.4f}\")  \n",
    "print(f\"PR AUC Score:  {average_precision_score(y_test, y_pred_proba):.4f}\")  \n",
    "print(f\"Accuracy:      {accuracy_score(y_test, y_pred):.4f}\") \n",
    "print(f\"Precision:     {precision_score(y_test, y_pred):.4f}\")  \n",
    "print(f\"Recall:        {recall_score(y_test, y_pred):.4f}\")  \n",
    "print(f\"F1 Score:      {f1_score(y_test, y_pred):.4f}\")  \n",
    "print(f\"MCC Score:     {matthews_corrcoef(y_test, y_pred):.4f}\")  \n",
    "\n",
    "print(\"\\n------- CONFUSION MATRIX -------\")  \n",
    "tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()  \n",
    "print(f\"True Negatives (Correct No-Claim): {tn}\")  \n",
    "print(f\"False Positives (False Alarm):     {fp}\") \n",
    "print(f\"False Negatives (Missed Claim):    {fn}\") \n",
    "print(f\"True Positives (Caught Claim):     {tp}\")  \n",
    "\n",
    "print(\"\\n------- CLASSIFICATION REPORT -------\")  \n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37be80a5-59fb-4b66-b422-19364ad61c62",
   "metadata": {},
   "source": [
    "## Project Conclusion\n",
    "\n",
    "The model can detect some customers who may file a claim, but performance is limited due to highly imbalanced data.\n",
    "\n",
    "ROC AUC of 0.6369 shows moderate probability prediction. PR AUC is very low, showing difficulty with rare claims.\n",
    "\n",
    "Precision is low (0.0554), recall is moderate (0.5434), and F1 is low (0.1005). MCC (0.0754) confirms impact of imbalance.\n",
    "\n",
    "Even without EDA, the model provides useful predictions.\n",
    "\n",
    "Future improvements can include handling data imbalance better, adding more feature information, or feature engineering to improve precision and recall.\n",
    "\n",
    "Summary: The project is workable and useful for risk management but is limited in performance due to dataset imbalance and lack of feature exploration."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
